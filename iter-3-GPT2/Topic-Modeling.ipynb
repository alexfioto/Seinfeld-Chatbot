{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexander.fioto/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKTopics(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator = 'LDA', n_topics=20):\n",
    "        self.n_topics = n_topics\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        elif estimator == 'NMF':\n",
    "            self.estimator = NMF(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_components=self.n_topics)\n",
    "        self.model = make_pipeline(CountVectorizer(stop_words='english', max_features=200), \n",
    "                                   LatentDirichletAllocation(n_components=self.n_topics))\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "        return self.model\n",
    "    \n",
    "    def get_topics(self, n = 25):\n",
    "        vectorizer = self.model.named_steps['countvectorizer']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n-1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alexander.fioto/personal_github/Seinfeld-Chatbot/iter-3-GPT2'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/episode_dialogues.pkl', 'rb') as f:\n",
    "    episode_dialogues = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_list = [values for key, values in episode_dialogues.items()]\n",
    "X = dialogue_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = SKTopics(estimator = 'LDA', n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_features=200, stop_words='english')),\n",
       "                ('latentdirichletallocation',\n",
       "                 LatentDirichletAllocation(n_components=20))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['baby', 'man', 'oh', 'look', 'll', 'kramer', 'just', 'like'],\n",
       " 1: ['yeah', 'know', 'oh', 'don', 'hey', 'gonna', 'like', 'just'],\n",
       " 2: ['im', 'yeah', 'hey', 'know', 'ah', 'ill', 'oh', 'just'],\n",
       " 3: ['yeah', 'know', 'oh', 'don', 'did', 'really', 'hey', 'like'],\n",
       " 4: ['car', 'know', 'hey', 'jerry', 'don', 'gonna', 'alright', 'kramer'],\n",
       " 5: ['funny', 'know', 'jerry', 'said', 'man', 'oh', 'yeah', 'don'],\n",
       " 6: ['jerry', 'apartment', 'alright', 'oh', 'newman', 'hello', 'want', 'time'],\n",
       " 7: ['looking', 'gave', 'hi', 'hey', 'help', 'hello', 'hell', 'heard'],\n",
       " 8: ['ha', 'newman', 'oh', 'know', 'mr', 'big', 'just', 'yeah'],\n",
       " 9: ['doctor', 'hey', 'going', 'oh', 'know', 'ya', 'don', 'good'],\n",
       " 10: ['looking', 'gave', 'hi', 'hey', 'help', 'hello', 'hell', 'heard'],\n",
       " 11: ['know', 'don', 'like', 'oh', 'll', 'going', 'got', 'right'],\n",
       " 12: ['oh', 'yeah', 'don', 'hey', 'know', 'just', 'like', 'right'],\n",
       " 13: ['oh', 'know', 'yeah', 'uh', 'alright', 'just', 'don', 'll'],\n",
       " 14: ['dont', 'know', 'im', 'just', 'oh', 'like', 'right', 'yeah'],\n",
       " 15: ['looking', 'gave', 'hi', 'hey', 'help', 'hello', 'hell', 'heard'],\n",
       " 16: ['know', 'don', 'like', 'oh', 'yeah', 'just', 'think', 'right'],\n",
       " 17: ['looking', 'gave', 'hi', 'hey', 'help', 'hello', 'hell', 'heard'],\n",
       " 18: ['oh', 'yeah', 'uh', 'know', 'jerry', 'hey', 'just', 'right'],\n",
       " 19: ['looking', 'gave', 'hi', 'hey', 'help', 'hello', 'hell', 'heard']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk.get_topics(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
